{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "aea793e7-7dd9-4e41-b35f-2004e09fc954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "b19da7fb-bdcd-4822-b320-5c4ae8069252",
   "metadata": {},
   "outputs": [],
   "source": [
    "Columns = pd.read_csv('Hourly_Column_Dictionary.csv') # Read column dictionary \n",
    "no_bound = (-np.inf,np.inf)\n",
    "bounds = dict.fromkeys(Columns.columns[4:], no_bound)\n",
    "\n",
    "def mask_outside(data, bounds):\n",
    "    lower, upper = bounds\n",
    "    data = data.mask(data< lower)\n",
    "    data = data.mask(data> upper)\n",
    "    return data\n",
    "\n",
    "bounds['Li190'] = (-10, 2700)\n",
    "bounds['Li200'] = (-10, 2700)\n",
    "bounds['Air_Temp'] = (-40, 45)\n",
    "bounds['5m_Temp'] = (-40, 45)\n",
    "bounds['12m_Temp'] = (-40, 45)\n",
    "bounds['Ground_Temp'] = (-40, 70)\n",
    "bounds['Rel_Humidity'] = (0, 110)\n",
    "bounds['Soil_Temp'] = (-40, 45)\n",
    "bounds['10cm_Soil_Temp'] = (-40, 45)\n",
    "bounds['20cm_Soil_Temp'] = (-40, 45)\n",
    "bounds['50cm_Soil_Temp'] = (-40, 45)\n",
    "bounds['Wind_Speed'] = (0,150)\n",
    "\n",
    "bounds['Li190_Min'] = (-10, 2700)\n",
    "bounds['Li200_Min'] = (-10, 2700)\n",
    "bounds['Air_Temp_Min'] = (-40, 45)\n",
    "bounds['5m_Temp_Min'] = (-40, 45)\n",
    "bounds['12m_Temp_Min'] = (-40, 45)\n",
    "bounds['Ground_Temp_Min'] = (-40, 70)\n",
    "bounds['Rel_Humidity_Min'] = (0, 110)\n",
    "bounds['Soil_Temp_Min'] = (-40, 45)\n",
    "bounds['10cm_Soil_Temp_Min'] = (-40, 45)\n",
    "bounds['20cm_Soil_Temp_Min'] = (-40, 45)\n",
    "bounds['50cm_Soil_Temp_Min'] = (-40, 45)\n",
    "bounds['Wind_Speed_Min'] = (0,150)\n",
    "\n",
    "bounds['Li190_Max'] = (-10, 2700)\n",
    "bounds['Li200_Max'] = (-10, 2700)\n",
    "bounds['Air_Temp_Max'] = (-40, 45)\n",
    "bounds['5m_Temp_Max'] = (-40, 45)\n",
    "bounds['12m_Temp_Max'] = (-40, 45)\n",
    "bounds['Ground_Temp_Max'] = (-40, 70)\n",
    "bounds['Rel_Humidity_Max'] = (0, 110)\n",
    "bounds['Soil_Temp_Max'] = (-40, 45)\n",
    "bounds['10cm_Soil_Temp_Max'] = (-40, 45)\n",
    "bounds['20cm_Soil_Temp_Max'] = (-40, 45)\n",
    "bounds['50cm_Soil_Temp_Max'] = (-40, 45)\n",
    "bounds['Wind_Speed_Max'] = (0,150)\n",
    "\n",
    "bounds['Wind_Dir'] = (0,360)\n",
    "bounds['Wind_Dir_STD'] = (0,np.inf)\n",
    "bounds['Rain'] = (0,1825)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b13c80d-4896-4158-b168-e9a73e24e942",
   "metadata": {},
   "source": [
    "## Reformat Cass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "7610e8db-6ace-4241-a9fe-bd5b73d78924",
   "metadata": {},
   "outputs": [],
   "source": [
    "Raw_Dir = r'Raw_Data/Cass_Hourly/' # Input \n",
    "\n",
    "Parsed_Dir = r'Formatted_Data/Cass_Hourly/' # Output\n",
    "if not os.path.exists(Parsed_Dir):\n",
    "    os.makedirs(Parsed_Dir)\n",
    "    \n",
    "Filtered_Dir = r'Filtered_Data/Cass_Hourly/' # Output\n",
    "if not os.path.exists(Filtered_Dir):\n",
    "    os.makedirs(Filtered_Dir)\n",
    "\n",
    "ymin = 1997 # Start of series\n",
    "ymax = 2020 # End of series\n",
    "years = range(ymin,ymax+1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "02ac3d8a-8894-4c0d-a67e-f50bfcd477b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n"
     ]
    }
   ],
   "source": [
    "for y in years:\n",
    "    print(y)\n",
    "    # Try to fetch data for that year\n",
    "    try:\n",
    "        data = pd.read_csv(Raw_Dir+'Cass_'+str(y)+'_Hourly.csv')\n",
    "    except:\n",
    "        print('No data for '+str(y))\n",
    "        continue\n",
    "    \n",
    "    # Create timeseries from time info\n",
    "    time = pd.to_datetime(data['Year'], format='%Y') + pd.to_timedelta(data.Day-1, unit='D') + pd.to_timedelta(data.Time/100, unit='H')\n",
    "    \n",
    "    # Index dataset by timeseries\n",
    "    data.index = time\n",
    "    data = data[~data.index.duplicated(keep='first')] # Drop any duplicate rows\n",
    "    \n",
    "    # Create container for formatted raw data\n",
    "    out_data = pd.DataFrame()\n",
    "    out_data.index = pd.date_range(dt.datetime(year=y,month=1,day=1,hour=1),dt.datetime(year=y+1,month=1,day=1),freq='H')\n",
    "    \n",
    "    # Create container for filtered data\n",
    "    filtered_data = pd.DataFrame()\n",
    "    filtered_data.index = pd.date_range(dt.datetime(year=y,month=1,day=1,hour=1),dt.datetime(year=y+1,month=1,day=1),freq='H')\n",
    "    \n",
    "    # Attach data to out_data\n",
    "    for C in Columns.columns[4:]:\n",
    "        for c in Columns[C]:\n",
    "            if c in list(data.columns):\n",
    "                try:\n",
    "                    # See if value can be interpreted as numeric\n",
    "                    out_data[C] = pd.to_numeric(data[c], errors='coerce')\n",
    "                    break\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    # Filter and attach data to filtered_data\n",
    "    for C in Columns.columns[4:]:\n",
    "        for c in Columns[C]:\n",
    "            if c in list(data.columns):\n",
    "                try:\n",
    "                    # See if value can be interpreted as numeric\n",
    "                    filtered_data[C] = mask_outside(pd.to_numeric(data[c], errors='coerce'), bounds[C])\n",
    "                    break\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "    out_data.index.names = ['Time'] # Set Index name to Time\n",
    "    out_data.to_csv(Parsed_Dir+'Cass_'+str(y)+'_Hourly.csv') # Save to file\n",
    "    \n",
    "    filtered_data.index.names = ['Time'] # Set Index name to Time\n",
    "    filtered_data.to_csv(Filtered_Dir+'Cass_'+str(y)+'_Hourly.csv') # Save to file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287064f4-a0f5-4089-93ea-097cdc330d88",
   "metadata": {},
   "source": [
    "## Reformat Chilton\n",
    "Chilton data is in two large xlsx files, so needs slightly different treatment to Cass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "ef51ae99-fffd-410c-9c9a-c3f6728cae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Raw_Dir = r'Raw_Data/Chilton_Hourly/' # Input \n",
    "Parsed_Dir = r'Formatted_Data/Chilton_Hourly/' # Output\n",
    "if not os.path.exists(Parsed_Dir):\n",
    "    os.makedirs(Parsed_Dir)\n",
    "\n",
    "Filtered_Dir = r'Filtered_Data/Chilton_Hourly/' # Output\n",
    "if not os.path.exists(Filtered_Dir):\n",
    "    os.makedirs(Filtered_Dir)\n",
    "\n",
    "files  = [f for f in os.listdir(Raw_Dir) if f.endswith('.xlsx')] # Find datafiles \n",
    "files.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "bf2b4ecc-f647-4475-b09d-1a7476d689d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Chilton_86_00_hourly.xlsx...\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "Reading Chilton_01_06_hourly.xlsx...\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n"
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    print('Reading '+file+'...')\n",
    "    data = pd.read_excel(Raw_Dir+file)\n",
    "    data.index = pd.to_datetime(data['date/time'],format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # Find timespan of dataset\n",
    "    ymin = min(data.index).year\n",
    "    ymax = max(data.index).year\n",
    "    years = range(ymin,ymax+1) \n",
    "    data = data[~data.index.duplicated(keep='first')] # Drop any duplicate rows\n",
    "    \n",
    "\n",
    "    for y in years:\n",
    "        print(y)\n",
    "        # Create container for data\n",
    "        out_data = pd.DataFrame()\n",
    "        out_data.index = pd.date_range(dt.datetime(year=y,month=1,day=1,hour=1),dt.datetime(year=y+1,month=1,day=1),freq='H')\n",
    "\n",
    "        # Create container for filtered data\n",
    "        filtered_data = pd.DataFrame()\n",
    "        filtered_data.index = pd.date_range(dt.datetime(year=y,month=1,day=1,hour=1),dt.datetime(year=y+1,month=1,day=1),freq='H')\n",
    "    \n",
    "        # Attach data to out_data\n",
    "        for C in Columns.columns[4:]:\n",
    "            for c in Columns[C]:\n",
    "                if c in list(data.columns):\n",
    "                    try:\n",
    "                        # See if value can be interpreted as numeric\n",
    "                        out_data[C] = pd.to_numeric(data[c], errors='coerce')\n",
    "                        break\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "        # Filter and attach data to filtered_data\n",
    "        for C in Columns.columns[4:]:\n",
    "            for c in Columns[C]:\n",
    "                if c in list(data.columns):\n",
    "                    try:\n",
    "                        # See if value can be interpreted as numeric\n",
    "                        filtered_data[C] = mask_outside(pd.to_numeric(data[c], errors='coerce'), bounds[C])\n",
    "                        break\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "        out_data.index.names = ['Time'] # Set Index name to Time\n",
    "        out_data.to_csv(Parsed_Dir+'Chilton_'+str(y)+'_Hourly.csv') # Save to file\n",
    "        \n",
    "        filtered_data.index.names = ['Time'] # Set Index name to Time\n",
    "        filtered_data.to_csv(Filtered_Dir+'Chilton_'+str(y)+'_Hourly.csv') # Save to file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dc27a9-afd4-4221-ba48-58b1e71d4786",
   "metadata": {},
   "source": [
    "### Mend solar data from chilton\n",
    "\n",
    "Chilton occasionally reads slightly less than 0 at night, this just retroactively passes a ammended timeseries to chilton with negative vals close to 0 set to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "55d92070-c76f-49a7-8b02-ab4211e01528",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in os.listdir('c:/Users/callu/Documents/GEOG Project/All_Data/Formatted_Data/Chilton_Hourly//') if f.endswith('.csv')]\n",
    "\n",
    "def mask_solar(data, bounds):\n",
    "    lower_lower, lower, upper = bounds\n",
    "    data = data.mask((data < lower) & (data > lower_lower), other=0)\n",
    "    data = data.mask(data < lower_lower)\n",
    "    data = data.mask(data > upper)\n",
    "    return data\n",
    "\n",
    "for f in files:\n",
    "    raw = pd.read_csv('c:/Users/callu/Documents/GEOG Project/All_Data/Formatted_Data/Chilton_Hourly//'+f)\n",
    "    filtered =  pd.read_csv('c:/Users/callu/Documents/GEOG Project/All_Data/Filtered_Data/Chilton_Hourly//'+f)\n",
    "    raw.index = raw.Time\n",
    "    filtered.index = raw.Time\n",
    "    \n",
    "    solar = mask_solar(raw.Li200, (-10,0,2700))\n",
    "    \n",
    "    filtered.Li200 = solar\n",
    "    filtered.to_csv('c:/Users/callu/Documents/GEOG Project/All_Data/Filtered_Data/Chilton_Hourly//'+f,index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565146fc-f460-4691-b333-0951b658e3a9",
   "metadata": {},
   "source": [
    "## Format Cliflo Data\n",
    "Similar to Chilton these are all found in big csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "755543c0-efe8-40bf-80e9-4c81cf363168",
   "metadata": {},
   "outputs": [],
   "source": [
    "Raw_Dir = r'Raw_Data/Cliflo_Hourly/' # Input \n",
    "Parsed_Dir = r'Formatted_Data/' # Output\n",
    "Filtered_Dir = r'Filtered_Data/' # Output\n",
    "\n",
    "Columns = pd.read_csv('Hourly_Column_Dictionary.csv') # Read column dictionary \n",
    "files  = [f for f in os.listdir(Raw_Dir) if f.endswith('.txt')] # Find datafiles \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "be1dd2ff-ce4d-4abe-a0b7-f68e0b194922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ArthursAWS_ScreenObs.txt...\n",
      "2012\n",
      "2013\n",
      "Reading ArthursEWS.txt...\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n"
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    print('Reading '+file+'...')\n",
    "    data = pd.read_csv(Raw_Dir+file)\n",
    "    data.index = pd.to_datetime(data['Date(NZST)'],format='%Y%m%d:%H%M')\n",
    "\n",
    "    # Find timespan of dataset\n",
    "    ymin = min(data.index).year\n",
    "    ymax = max(data.index).year\n",
    "    years = range(ymin,ymax+1) \n",
    "    data = data[~data.index.duplicated(keep='first')] # Drop any duplicate rows\n",
    "\n",
    "    for y in years:\n",
    "        print(y)\n",
    "        # Create container for data\n",
    "        out_data = pd.DataFrame()\n",
    "        out_data.index = pd.date_range(dt.datetime(year=y,month=1,day=1,hour=1),dt.datetime(year=y+1,month=1,day=1),freq='H')\n",
    "\n",
    "        # Create container for filtered data\n",
    "        filtered_data = pd.DataFrame()\n",
    "        filtered_data.index = pd.date_range(dt.datetime(year=y,month=1,day=1,hour=1),dt.datetime(year=y+1,month=1,day=1),freq='H')\n",
    "        \n",
    "        # Attach data to out_data\n",
    "        for C in Columns.columns[4:]:\n",
    "            for c in Columns[C]:\n",
    "                if c in list(data.columns):\n",
    "                    try:\n",
    "                        # See if value can be interpreted as numeric\n",
    "                        out_data[C] = pd.to_numeric(data[c], errors='coerce')\n",
    "                        break\n",
    "                    except:\n",
    "                        pass\n",
    "            \n",
    "        # Filter and attach data to filtered_data\n",
    "        for C in Columns.columns[4:]:\n",
    "            for c in Columns[C]:\n",
    "                if c in list(data.columns):\n",
    "                    try:\n",
    "                        # See if value can be interpreted as numeric\n",
    "                        filtered_data[C] = mask_outside(pd.to_numeric(data[c], errors='coerce'), bounds[C])\n",
    "                        break\n",
    "                    except:\n",
    "                        pass\n",
    "        out_data.index.names = ['Time'] # Set Index name to Time'\n",
    "        filtered_data.index.names = ['Time'] # Set Index name to Time\n",
    "        \n",
    "        station = file[:-4]\n",
    "        \n",
    "        if not os.path.exists(Parsed_Dir+station+'_Hourly/'):\n",
    "            os.makedirs(Parsed_Dir+station+'_Hourly/')\n",
    "        if not os.path.exists(Filtered_Dir+station+'_Hourly/'):\n",
    "            os.makedirs(Filtered_Dir+station+'_Hourly/')\n",
    "        \n",
    "        out_data.to_csv(Parsed_Dir+station+'_Hourly/'+station+'_'+str(y)+'_Hourly.csv') # Save to file\n",
    "        out_data.to_csv(Filtered_Dir+station+'_Hourly/'+station+'_'+str(y)+'_Hourly.csv') # Save to file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
