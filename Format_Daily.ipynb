{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da3aa476-b4a9-4f55-8f0e-97aad2a8b221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "import datetime as dt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ac8f0b-7d1a-4d0b-bd87-5e0e070744d7",
   "metadata": {},
   "source": [
    "## Cass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a14875c8-8b8a-43f9-8d38-4223ea4740fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Columns = pd.read_csv(r'Daily_Column_Dictionary.csv')\n",
    "\n",
    "no_bound = (-np.inf,np.inf)\n",
    "bounds = dict.fromkeys(Columns.columns[4:], no_bound)\n",
    "\n",
    "def mask_outside(data, bounds):\n",
    "    lower, upper = bounds\n",
    "    data = data.mask(data< lower)\n",
    "    data = data.mask(data> upper)\n",
    "    return data\n",
    "\n",
    "bounds['Li190'] = (-10, 2700)\n",
    "bounds['Li200'] = (-10, 1300)\n",
    "bounds['Air_Temp'] = (-40, 45)\n",
    "bounds['5m_Temp'] = (-40, 45)\n",
    "bounds['12m_Temp'] = (-40, 45)\n",
    "bounds['Ground_Temp'] = (-40, 70)\n",
    "bounds['Rel_Humidity'] = (0, 110)\n",
    "bounds['Soil_Temp'] = (-40, 45)\n",
    "bounds['10cm_Soil_Temp'] = (-40, 45)\n",
    "bounds['20cm_Soil_Temp'] = (-40, 45)\n",
    "bounds['50cm_Soil_Temp'] = (-40, 45)\n",
    "bounds['Wind_Speed'] = (0,150)\n",
    "\n",
    "bounds['Li190_Min'] = (-10, 2700)\n",
    "bounds['Li200_Min'] = (-10, 2700)\n",
    "bounds['Air_Temp_Min'] = (-40, 45)\n",
    "bounds['5m_Temp_Min'] = (-40, 45)\n",
    "bounds['12m_Temp_Min'] = (-40, 45)\n",
    "bounds['Ground_Temp_Min'] = (-40, 70)\n",
    "bounds['Rel_Humidity_Min'] = (0, 110)\n",
    "bounds['Soil_Temp_Min'] = (-40, 45)\n",
    "bounds['10cm_Soil_Temp_Min'] = (-40, 45)\n",
    "bounds['20cm_Soil_Temp_Min'] = (-40, 45)\n",
    "bounds['50cm_Soil_Temp_Min'] = (-40, 45)\n",
    "bounds['Wind_Speed_Min'] = (0,150)\n",
    "\n",
    "bounds['Li190_Max'] = (-10, 2700)\n",
    "bounds['Li200_Max'] = (-10, 2700)\n",
    "bounds['Air_Temp_Max'] = (-40, 45)\n",
    "bounds['5m_Temp_Max'] = (-40, 45)\n",
    "bounds['12m_Temp_Max'] = (-40, 45)\n",
    "bounds['Ground_Temp_Max'] = (-40, 70)\n",
    "bounds['Rel_Humidity_Max'] = (0, 110)\n",
    "bounds['Soil_Temp_Max'] = (-40, 45)\n",
    "bounds['10cm_Soil_Temp_Max'] = (-40, 45)\n",
    "bounds['20cm_Soil_Temp_Max'] = (-40, 45)\n",
    "bounds['50cm_Soil_Temp_Max'] = (-40, 45)\n",
    "bounds['Wind_Speed_Max'] = (0,150)\n",
    "\n",
    "bounds['Wind_Dir'] = (0,360)\n",
    "bounds['Wind_Dir_STD'] = (0,np.inf)\n",
    "bounds['Rain'] = (0,1825)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a53601-edc2-4d7e-bb88-6de4288da4ce",
   "metadata": {},
   "source": [
    "## Cass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ca8442f-feea-46dd-b590-5961d4198c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "Raw_Dir = r'Raw_Data/Cass_Daily/' # Input \n",
    "\n",
    "Parsed_Dir = r'Formatted_Data/Cass_Daily/' # Output\n",
    "if not os.path.exists(Parsed_Dir):\n",
    "    os.makedirs(Parsed_Dir)\n",
    "    \n",
    "Filtered_Dir = r'Filtered_Data/Cass_Daily/' # Output\n",
    "if not os.path.exists(Filtered_Dir):\n",
    "    os.makedirs(Filtered_Dir)\n",
    "    \n",
    "ymin = 1997 # Start of series\n",
    "ymax = 2020 # End of series\n",
    "years = range(ymin,ymax+1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f350ef00-a48e-4735-a01a-d05ff9632932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\callu\\AppData\\Local\\Temp\\ipykernel_20164\\2377194124.py:18: FutureWarning: reindexing with a non-unique Index is deprecated and will raise in a future version.\n",
      "  out_data[C] = pd.to_numeric(data[c], errors='coerce')\n",
      "C:\\Users\\callu\\AppData\\Local\\Temp\\ipykernel_20164\\2377194124.py:29: FutureWarning: reindexing with a non-unique Index is deprecated and will raise in a future version.\n",
      "  filtered_data[C] = mask_outside(pd.to_numeric(data[c], errors='coerce'), bounds[C])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018\n",
      "2019\n",
      "2020\n"
     ]
    }
   ],
   "source": [
    "for y in years:\n",
    "    print(y)\n",
    "    data = pd.read_csv(Raw_Dir+'/Cass_'+str(y)+'_Daily.csv')\n",
    "    data = data[~data.index.duplicated(keep='first')]\n",
    "    out_data = pd.DataFrame()\n",
    "    filtered_data = pd.DataFrame()\n",
    "\n",
    "    data.index = pd.to_datetime(data.Year,format='%Y')+pd.to_timedelta(data.Day-1,unit='D')\n",
    "    out_data.index = pd.date_range(dt.datetime(y,1,1),dt.datetime(y,12,31))\n",
    "    filtered_data.index = pd.date_range(dt.datetime(y,1,1),dt.datetime(y,12,31))\n",
    "\n",
    "    # Attach data to out_data\n",
    "    for C in Columns.columns[4:]:\n",
    "        for c in Columns[C]:\n",
    "            if c in list(data.columns):\n",
    "                try:\n",
    "                    # See if value can be interpreted as numeric\n",
    "                    out_data[C] = pd.to_numeric(data[c], errors='coerce')\n",
    "                    break\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    # Filter and attach data to filtered_data\n",
    "    for C in Columns.columns[4:]:\n",
    "        for c in Columns[C]:\n",
    "            if c in list(data.columns):\n",
    "                try:\n",
    "                    # See if value can be interpreted as numeric\n",
    "                    filtered_data[C] = mask_outside(pd.to_numeric(data[c], errors='coerce'), bounds[C])\n",
    "                    break\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "    out_data.index.names = ['Time'] # Set Index name to Time\n",
    "    out_data.to_csv(Parsed_Dir+'Cass_'+str(y)+'_Daily.csv') # Save to file\n",
    "    \n",
    "    filtered_data.index.names = ['Time']\n",
    "    filtered_data.to_csv(Filtered_Dir+'/Cass_'+str(y)+'_Daily.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eb76f7-c539-4224-a70a-a062bda5f43c",
   "metadata": {},
   "source": [
    "## Reformat Chilton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "80360244-f591-44a0-97bb-70d79bb18a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Raw_Dir = r'Raw_Data/Chilton_Daily/' # Input \n",
    "Parsed_Dir = 'Formatted_Data/Chilton_Daily/' # Output\n",
    "if not os.path.exists(Parsed_Dir):\n",
    "    os.makedirs(Parsed_Dir)\n",
    "\n",
    "Filtered_Dir = 'Filtered_Data/Chilton_Daily/' # Output\n",
    "if not os.path.exists(Filtered_Dir):\n",
    "    os.makedirs(Filtered_Dir)\n",
    "\n",
    "files  = [f for f in os.listdir(Raw_Dir) if f.endswith('.csv')] # Find datafiles \n",
    "files.reverse()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bd18ac6f-4772-4d77-b4d0-f72a3fed8b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Chilton_Daily_86_00.csv...\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "Reading Chilton_Daily_01_06.csv...\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n"
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    print('Reading '+file+'...')\n",
    "    data = pd.read_csv(Raw_Dir+file)\n",
    "    data.index = pd.to_datetime(data['date'],format='%m/%d/%Y')\n",
    "\n",
    "    # Find timespan of dataset\n",
    "    ymin = min(data.index).year\n",
    "    ymax = max(data.index).year\n",
    "    years = range(ymin,ymax+1) \n",
    "    data = data[~data.index.duplicated(keep='first')] # Drop any duplicate rows\n",
    "    \n",
    "\n",
    "    for y in years:\n",
    "        print(y)\n",
    "        # Create container for data\n",
    "        out_data = pd.DataFrame()\n",
    "        out_data.index = pd.date_range(dt.datetime(year=y,month=1,day=1),dt.datetime(year=y+1,month=1,day=1),freq='D')\n",
    "\n",
    "        # Create container for filtered data\n",
    "        filtered_data = pd.DataFrame()\n",
    "        filtered_data.index = pd.date_range(dt.datetime(year=y,month=1,day=1),dt.datetime(year=y+1,month=1,day=1),freq='D')\n",
    "    \n",
    "        # Attach data to out_data\n",
    "        for C in Columns.columns[4:]:\n",
    "            for c in Columns[C]:\n",
    "                if c in list(data.columns):\n",
    "                    try:\n",
    "                        # See if value can be interpreted as numeric\n",
    "                        out_data[C] = pd.to_numeric(data[c], errors='coerce')\n",
    "                        break\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "        # Filter and attach data to filtered_data\n",
    "        for C in Columns.columns[4:]:\n",
    "            for c in Columns[C]:\n",
    "                if c in list(data.columns):\n",
    "                    try:\n",
    "                        # See if value can be interpreted as numeric\n",
    "                        filtered_data[C] = mask_outside(pd.to_numeric(data[c], errors='coerce'), bounds[C])\n",
    "                        break\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "        out_data.index.names = ['Time'] # Set Index name to Time\n",
    "        out_data.to_csv(Parsed_Dir+'Chilton_'+str(y)+'_Daily.csv') # Save to file\n",
    "        \n",
    "        filtered_data.index.names = ['Time'] # Set Index name to Time\n",
    "        filtered_data.to_csv(Filtered_Dir+'Chilton_'+str(y)+'_Daily.csv') # Save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a0f04b-7535-410c-8d14-f3dde24cb4af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time\n",
       "2006-01-01   NaN\n",
       "2006-01-02   NaN\n",
       "2006-01-03   NaN\n",
       "2006-01-04   NaN\n",
       "2006-01-05   NaN\n",
       "              ..\n",
       "2006-12-28   NaN\n",
       "2006-12-29   NaN\n",
       "2006-12-30   NaN\n",
       "2006-12-31   NaN\n",
       "2007-01-01   NaN\n",
       "Freq: D, Name: Air_Temp_Min, Length: 366, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_outside(pd.to_numeric(out_data.Air_Temp_Min, errors='coerce'),(0,19))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5001b072-a97a-463d-b67a-c7baf134b5c3",
   "metadata": {},
   "source": [
    "## Sync daily with cleaned hourly readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a7a32591-6451-4330-b0b2-37c6f56a670d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanHourly_dirtyDaily(hourly_path, daily_path, station_name, out_path, years):\n",
    "    \n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)\n",
    "    \n",
    "    for y in years:\n",
    "        print(y,end='\\r')\n",
    "        cleaned_data = pd.read_csv(hourly_path+station_name+'_'+str(y)+'_Hourly.csv')\n",
    "        data = pd.read_csv(daily_path+station_name+'_'+str(y)+'_Daily.csv')\n",
    "\n",
    "        data.Time = pd.to_datetime(data.Time,format='%Y-%m-%d')\n",
    "        cleaned_data.Time = pd.to_datetime(cleaned_data.Time)\n",
    "        out_data = data.copy()\n",
    "\n",
    "        for i, day in enumerate(data.Time):\n",
    "            hours = pd.date_range(day+dt.timedelta(hours=-23), day+dt.timedelta(hours=0),freq='H').tolist()\n",
    "            day_data = cleaned_data[cleaned_data.Time.isin(hours)]\n",
    "            for var in cleaned_data.columns.tolist():\n",
    "                daily_vars = [c for c in data.columns if c.startswith(var)]\n",
    "                if any(day_data[var].isna()):\n",
    "                    for daily_var in daily_vars:\n",
    "                        out_data.loc[i,daily_var]=np.nan\n",
    "\n",
    "\n",
    "        out_data.to_csv(out_path+'/'+station_name+'_'+str(y)+'_Daily.csv',index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "68facf14-b317-4429-a732-a7422abf2f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006\r"
     ]
    }
   ],
   "source": [
    "hourly_path = 'Cleaned_Data/Chilton_Hourly/'\n",
    "daily_path = 'Filtered_Data/Chilton_Daily/'\n",
    "station_name = 'Chilton'\n",
    "out_path = 'Cleaned_Data/Chilton_Daily/'\n",
    "years = range(1986,2007)\n",
    "cleanHourly_dirtyDaily(hourly_path, daily_path, station_name, out_path, years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8a07e06c-50df-49a1-851b-5bc53b3a1684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019\r"
     ]
    }
   ],
   "source": [
    "hourly_path = 'Cleaned_Data/Cass_Hourly/'\n",
    "daily_path = 'Filtered_Data/Cass_Daily/'\n",
    "station_name = 'Cass'\n",
    "out_path = 'Cleaned_Data/Cass_Daily/'\n",
    "years = range(1997,2020)\n",
    "cleanHourly_dirtyDaily(hourly_path, daily_path, station_name, out_path, years)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e2c5ca-01d6-469f-84c1-1afbe70b8b5f",
   "metadata": {},
   "source": [
    "## Parse and filter ECAN rain gauge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f7ea92f9-d90e-4301-b655-1683547c5644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ECAN_rain(datapath, Parsed_Dir, Filtered_Dir):\n",
    "    data  = pd.read_csv(datapath)\n",
    "    data.Time = pd.to_datetime(data.Time,format='%d/%m/%Y %H:%M')\n",
    "    years = range(min(pd.to_datetime(data.Time,format='%d/%m/%Y %H:%M')).year,\n",
    "                  max(pd.to_datetime(data.Time,format='%d/%m/%Y %H:%M')).year+1)\n",
    "    for y in years:\n",
    "        print(y, end='\\r')\n",
    "        \n",
    "        out_data = pd.DataFrame()\n",
    "        out_data.index = pd.date_range(dt.datetime(y,1,1),dt.datetime(y,12,31))\n",
    "        data.index = data.Time\n",
    "        data.index.names = ['Time']\n",
    "\n",
    "        out_data = pd.DataFrame()\n",
    "        out_data.index = pd.date_range(dt.datetime(year=y,month=1,day=1),dt.datetime(year=y+1,month=1,day=1),freq='D')\n",
    "\n",
    "        # Create container for filtered data\n",
    "        filtered_data = pd.DataFrame()\n",
    "        filtered_data.index = pd.date_range(dt.datetime(year=y,month=1,day=1),dt.datetime(year=y+1,month=1,day=1),freq='D')\n",
    "    \n",
    "        # Attach data to out_data\n",
    "        for C in Columns.columns[4:]:\n",
    "            for c in Columns[C]:\n",
    "                if c in list(data.columns):\n",
    "                    try:\n",
    "                        # See if value can be interpreted as numeric\n",
    "                        out_data[C] = pd.to_numeric(data[c], errors='coerce')\n",
    "                        break\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "        # Filter and attach data to filtered_data\n",
    "        for C in Columns.columns[4:]:\n",
    "            for c in Columns[C]:\n",
    "                if c in list(data.columns):\n",
    "                    try:\n",
    "                        # See if value can be interpreted as numeric\n",
    "                        filtered_data[C] = mask_outside(pd.to_numeric(data[c], errors='coerce'), bounds[C])\n",
    "                        break\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "        out_data.index.names = ['Time'] # Set Index name to Time\n",
    "        out_data.to_csv(Parsed_Dir+'_'+str(y)+'_Daily.csv') # Save to file\n",
    "        \n",
    "        filtered_data.index.names = ['Time'] # Set Index name to Time\n",
    "        filtered_data.to_csv(Filtered_Dir+'_'+str(y)+'_Daily.csv') # Save to file\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d56ca2ca-15a8-4fca-8424-f9cac31ba056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022\r"
     ]
    }
   ],
   "source": [
    "Stations = ['ArthursRain','CarringtonRain','GrasmereRain','EskRain']\n",
    "for f in Stations:\n",
    "    datapath = 'Raw_Data/'+f+'/'+f+'.csv'\n",
    "    Parsed_Dir = 'Formatted_Data/'+f+'_Daily/'+f\n",
    "    Filtered_Dir = 'Filtered_Data/'+f+'_Daily/'+f\n",
    "    \n",
    "    if not os.path.exists('Formatted_Data/'+f+'_Daily/'):\n",
    "        os.makedirs('Formatted_Data/'+f+'_Daily/')\n",
    "    if not os.path.exists('Filtered_Data/'+f+'_Daily/'):\n",
    "        os.makedirs('Filtered_Data/'+f+'_Daily/')\n",
    "    \n",
    "    parse_ECAN_rain(datapath, Parsed_Dir, Filtered_Dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc6019d-ab60-4e41-a925-ecaac1edb0d4",
   "metadata": {},
   "source": [
    "## Format Cliflo Datasets\n",
    "Cliflo datasets are all in a universal format, as such there is a function called Parse_Cliflo in the Database_Utils.py library for you to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83fc622e-da4c-4bef-9d24-69621b438d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ArthursCombined',\n",
       " 'ArthursEWS',\n",
       " 'ArthursStoreRain',\n",
       " 'ArthursTemps',\n",
       " 'ArthursWind',\n",
       " 'BealeyRain',\n",
       " 'BrokenRiverCombined',\n",
       " 'BrokenRiverRain',\n",
       " 'BrokenRiverTemp',\n",
       " 'BrokenRiverWind',\n",
       " 'CampStreamCombined',\n",
       " 'CampStreamRain',\n",
       " 'CampStreamWind',\n",
       " 'CastleHillRain',\n",
       " 'ClifloArthursRain',\n",
       " 'CragieburnForestCombined',\n",
       " 'CragieburnForestTemps',\n",
       " 'CragieburnForestWind',\n",
       " 'CragieburnStnRain',\n",
       " 'FlockhillRain',\n",
       " 'HarperRiverTemps',\n",
       " 'MtPhilistineEWSCombined',\n",
       " 'MtPhilistineTemps',\n",
       " 'MtWhiteRain',\n",
       " 'OldCassRain',\n",
       " 'OtiraTemps']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ca37d3c-259e-4227-9225-b62fde6237dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1987\r"
     ]
    }
   ],
   "source": [
    "# Get a list of all the stations in /Raw_Data/Cliflo_Daily\n",
    "Stations = [f[:-4][:-6] for f in os.listdir('Raw_Data/Cliflo_Daily/') if f.endswith('txt')]\n",
    "\n",
    "import Database_Utils as DU\n",
    "\n",
    "for f in Stations:\n",
    "    datapath = 'Raw_Data/Cliflo_Daily/'+f+'_Daily.txt'\n",
    "    Parsed_Dir = 'Formatted_Data/'+f+'_Daily/'+f\n",
    "    Filtered_Dir = 'Filtered_Data/'+f+'_Daily/'+f\n",
    "    \n",
    "    if not os.path.exists('Formatted_Data/'+f+'_Daily/'):\n",
    "        os.makedirs('Formatted_Data/'+f+'_Daily/')\n",
    "    if not os.path.exists('Filtered_Data/'+f+'_Daily/'):\n",
    "        os.makedirs('Filtered_Data/'+f+'_Daily/')\n",
    "    \n",
    "    DU.Parse_Cliflo(datapath, Parsed_Dir, Filtered_Dir, Columns, bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5700b1f-949e-40ce-be44-d4db08f4eb03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CragieburnForestRain'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab76b49-4564-4879-91f9-f5238e990baf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
